{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn"
   ],
   "id": "8e06de36ae1844"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 伪代码\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "de3742ef1830170d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 伪代码\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "71b4aa1c03cf31bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 完整公式总结\n",
    "\n",
    "$\\mathbf{P} = \\text{softmax}\\left(\n",
    "    \\text{Decoder}\\left(\n",
    "        \\mathbf{X}_{\\text{trg}},\n",
    "        \\text{Encoder}(\\mathbf{X}_{\\text{src}}, \\mathbf{M}_{\\text{src}}),\n",
    "        \\mathbf{M}_{\\text{trg}},\n",
    "        \\mathbf{M}_{\\text{enc\\_dec}}\n",
    "    \\right) \\cdot \\mathbf{W}_{\\text{out}}\n",
    "\\right)$\n",
    "\n",
    "其中各个掩码的定义如上所述。\n",
    "\n",
    "---\n",
    "\n",
    "### 连接点的核心公式\n",
    "\n",
    "编码器和解码器连接的核心体现在解码器的交叉注意力层：\n",
    "\n",
    "$\\text{CrossAttention} = \\text{softmax}\\left(\n",
    "    \\frac{(\\mathbf{H}_{\\text{dec}}\\mathbf{W}_{Q,\\text{cross}})(\\mathbf{H}_{\\text{enc}}\\mathbf{W}_{K,\\text{cross}})^\\top}{\\sqrt{d_k}} + \\mathbf{M}_{\\text{enc\\_dec}}\n",
    "\\right) \\cdot (\\mathbf{H}_{\\text{enc}}\\mathbf{W}_{V,\\text{cross}})$"
   ],
   "id": "b605779a7869ea9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,src_pad_idx,trg_pad_idx,enc_voc_size,dec_voc_size,d_model,max_len,n_head,ffn_hidden,n_layer,drop_prob,device):\n",
    "        super().__init__()\n",
    "        # 定义decoder和encoder\n",
    "        self.encoder=Encoder(enc_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,device,drop_prob)\n",
    "        self.decoder=Decoder(dec_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,drop_prob,device)\n",
    "        # 添加属性\n",
    "        self.src_pad_idx=src_pad_idx\n",
    "        self.trg_pad_idx=trg_pad_idx\n",
    "        self.device=device\n",
    "    # 生成mask的函数\n",
    "    def make_pad_mask(self,q:torch.Tensor,k:torch.Tensor,pad_idx_q,pad_idx_k):\n",
    "        len_q,len_k=q.size(1),k.size(1)\n",
    "        q=q.ne(pad_idx_q).unsqueeze(1).unsqueeze(3) # q的shape是(batch_size,1,len_q,1)\n",
    "        q=q.repeat(1,1,1,len_k) # q的shape是(batch_size,1,len_q,len_k)\n",
    "        k=k.ne(pad_idx_k).unsqueeze(1).unsqueeze(2) # k的shape是(batch_size,1,1,len_k)\n",
    "        k=k.repeat(1,1,len_q,1) # k的shape是(batch_size,1,len_q,len_k)\n",
    "        mask=q&k # mask的shape为(batch_size,1,len_q,len_k)\n",
    "        return mask\n",
    "    # 构建因果掩码\n",
    "    def make_casual_mask(self,q:torch.Tensor,k:torch.Tensor):\n",
    "        len_q,len_k=q.size(1),k.size(1)\n",
    "        batch_size=q.size(0)\n",
    "        mask=torch.tril(torch.ones(size=(len_q,len_k)).to(dtype=torch.bool,device=torch.device(self.device)).unsqueeze(0).unsqueeze(1).repeat(batch_size,1,1,1))\n",
    "        return mask # mask的shape为(len_q,len_k)\n",
    "    def forward(self,src,trg):\n",
    "        src_mask=self.make_pad_mask(src,src,self.src_pad_idx,self.src_pad_idx)\n",
    "        trg_mask=self.make_pad_mask(trg,trg,self.trg_pad_idx,self.trg_pad_idx)&self.make_casual_mask(trg,trg)\n",
    "        enc=self.encoder(src,src_mask)\n",
    "        out=self.decoder(trg,enc,trg_mask,src_mask)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
