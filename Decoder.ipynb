{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-08T03:30:09.446933Z",
     "start_time": "2025-09-08T03:30:07.572966Z"
    }
   },
   "source": [
    "from os import device_encoding\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:30:09.838208Z",
     "start_time": "2025-09-08T03:30:09.831882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ä¼ªä»£ç \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "a6b802e2fd8129be",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:30:09.858077Z",
     "start_time": "2025-09-08T03:30:09.853241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ä¼ªä»£ç \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "c857ca57e6df7116",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:35:38.614710Z",
     "start_time": "2025-09-08T03:35:38.600307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ä¼ªä»£ç \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "f9058eae1e6682c6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ä¼ªä»£ç \n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "6ba2caddd27be9c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* è¾“å…¥å‘é‡ï¼š$\\mathbf{x}$\n",
    "* Encoder è¾“å‡ºï¼ˆè®°ä½œ memoryï¼‰ï¼š$\\mathbf{M}$\n",
    "* å„ä¸ªå­å±‚çš„ç»“æœï¼šä¾æ¬¡æ˜¯ $\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{y}$\n",
    "\n",
    "## DecoderLayer çš„å…¬å¼\n",
    "\n",
    "### 1. Masked Multi-Head Self-Attention\n",
    "\n",
    "å¯¹ç›®æ ‡åºåˆ—çš„è¾“å…¥ $\\mathbf{x}$ï¼Œåšå¸¦æ©ç çš„å¤šå¤´æ³¨æ„åŠ›ï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_1 = \\text{MultiHeadAttention}(\\mathbf{x}, \\mathbf{x}, \\mathbf{x}, \\text{mask})\n",
    "$$\n",
    "\n",
    "åŠ ä¸Šæ®‹å·® & LayerNormï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_1 = \\text{LayerNorm}(\\mathbf{x} + \\mathbf{z}_1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Encoderâ€“Decoder Attention (Cross-Attention)\n",
    "\n",
    "ä»¥ $\\mathbf{x}_1$ ä½œä¸º Queryï¼ŒEncoder çš„è¾“å‡º $\\mathbf{M}$ ä½œä¸º Key å’Œ Valueï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_2 = \\text{MultiHeadAttention}(\\mathbf{x}_1, \\mathbf{M}, \\mathbf{M})\n",
    "$$\n",
    "\n",
    "åŠ ä¸Šæ®‹å·® & LayerNormï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_2 = \\text{LayerNorm}(\\mathbf{x}_1 + \\mathbf{z}_2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Position-wise Feed Forward Network (FFN)\n",
    "\n",
    "FFN é€šå¸¸æ˜¯ä¸¤å±‚å‰é¦ˆç½‘ç»œï¼š\n",
    "\n",
    "$$\n",
    "\\text{FFN}(\\mathbf{x}_2) = \\max(0, \\mathbf{x}_2 \\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2\n",
    "$$\n",
    "\n",
    "åŠ ä¸Šæ®‹å·® & LayerNormï¼š\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x}_2 + \\text{FFN}(\\mathbf{x}_2))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Head Attention çš„ç»†èŠ‚å…¬å¼\n",
    "\n",
    "æ¯ä¸ª **æ³¨æ„åŠ›å¤´ï¼ˆheadï¼‰**ï¼š\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + \\text{mask}\\right)V\n",
    "$$\n",
    "\n",
    "å¤šå¤´æ‹¼æ¥åï¼š\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)\\mathbf{W}^O\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q\\mathbf{W}^Q_i, K\\mathbf{W}^K_i, V\\mathbf{W}^V_i)\n",
    "$$"
   ],
   "id": "781cef7d4db52c73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:30:09.932597Z",
     "start_time": "2025-09-08T03:30:09.924952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,drop_prod,n_head):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.ffn_hidden=ffn_hidden\n",
    "        self.drop_prod=drop_prod\n",
    "        self.n_head=n_head\n",
    "        # Masked Self-Attention\n",
    "        self.self_attention=MultiHeadAttention(self.d_model,self.n_head)\n",
    "        # Cross Attention\n",
    "        self.cross_attention=MultiHeadAttention(self.d_model,self.n_head)\n",
    "        # Position-Wise-Feed-Forward\n",
    "        self.ffn=PositionWiseFeedForward(self.d_model,self.ffn_hidden,self.drop_prod)\n",
    "        # LayerNorm\n",
    "        self.norm1=LayerNorm(self.d_model)\n",
    "        self.norm2=LayerNorm(self.d_model)\n",
    "        self.norm3=LayerNorm(self.d_model)\n",
    "        # Dropout\n",
    "        self.dropout1=nn.Dropout(self.drop_prod)\n",
    "        self.dropout2=nn.Dropout(self.drop_prod)\n",
    "        self.dropout3=nn.Dropout(self.drop_prod)\n",
    "    def forward(self,dec,enc,t_mask,s_mask):\n",
    "        x1=self.norm1(self.dropout1(self.self_attention(dec,dec,dec,t_mask))+dec)\n",
    "        x2=self.norm2(self.dropout2(self.cross_attention(x1,enc,enc,s_mask)+x1))\n",
    "        y=self.norm3(x2+self.dropout3(self.ffn(x2)))\n",
    "        return y"
   ],
   "id": "49ccfe14f8cd6c26",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "å¥½çš„ï¼ä»¥ä¸‹æ˜¯ Transformer Decoder éƒ¨åˆ†çš„è¯¦ç»†è®¡ç®—å…¬å¼ã€‚æˆ‘ä»¬å°†åˆ†æ­¥éª¤è§£ææ¯ä¸ªç»„ä»¶çš„æ•°å­¦è¡¨è¾¾ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## Decoder æ•´ä½“è®¡ç®—æµç¨‹\n",
    "\n",
    "è®¾ï¼š\n",
    "- `X`ï¼šç›®æ ‡åºåˆ—çš„è¯åµŒå…¥ï¼ˆshifted rightï¼‰ + ä½ç½®ç¼–ç ï¼Œå½¢çŠ¶ä¸º `(batch_size, tgt_len, d_model)`\n",
    "- `Z`ï¼šEncoder çš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º `(batch_size, src_len, d_model)`\n",
    "- `M_t`ï¼šç›®æ ‡åºåˆ—çš„æ©ç çŸ©é˜µï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Masked Multi-Head Self-Attention\n",
    "\n",
    "è¿™æ˜¯ Decoder çš„ç¬¬ä¸€å­å±‚ï¼Œç¡®ä¿è‡ªå›å½’æ€§è´¨ã€‚\n",
    "\n",
    "### è®¡ç®—å…¬å¼ï¼š\n",
    "\n",
    "**a) çº¿æ€§å˜æ¢å¾—åˆ° Q, K, Vï¼š**\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q &= X \\cdot W_Q \\\\\n",
    "K &= X \\cdot W_K \\\\\n",
    "V &= X \\cdot W_V\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**b) Scaled Dot-Product Attentionï¼ˆå¸¦æ©ç ï¼‰ï¼š**\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M_t\\right)V\n",
    "$$\n",
    "\n",
    "**c) Multi-Head æ‹¼æ¥ï¼š**\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W_O\n",
    "$$\n",
    "å…¶ä¸­ $\\text{head}_i = \\text{Attention}(XW_Q^i, XW_K^i, XW_V^i)$\n",
    "\n",
    "**d) Add & Normï¼š**\n",
    "$$\n",
    "X_1 = \\text{LayerNorm}(X + \\text{Dropout}(\\text{MultiHead}(X, X, X)))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Multi-Head Cross-Attention\n",
    "\n",
    "è¿™æ˜¯è¿æ¥ Encoder å’Œ Decoder çš„å…³é”®æ¡¥æ¢ã€‚\n",
    "\n",
    "### è®¡ç®—å…¬å¼ï¼š\n",
    "\n",
    "**a) çº¿æ€§å˜æ¢ï¼ˆQæ¥è‡ªDecoderï¼ŒK,Væ¥è‡ªEncoderï¼‰ï¼š**\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q &= X_1 \\cdot W_Q \\\\\n",
    "K &= Z \\cdot W_K \\\\\n",
    "V &= Z \\cdot W_V\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**b) Scaled Dot-Product Attentionï¼š**\n",
    "$$\n",
    "\\text{CrossAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "**c) Multi-Head æ‹¼æ¥ï¼š**\n",
    "$$\n",
    "\\text{MultiHeadCross}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W_O\n",
    "$$\n",
    "\n",
    "**d) Add & Normï¼š**\n",
    "$$\n",
    "X_2 = \\text{LayerNorm}(X_1 + \\text{Dropout}(\\text{MultiHeadCross}(X_1, Z, Z)))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Position-wise Feed-Forward Network\n",
    "\n",
    "ä¸å‰ä¸¤ä¸ªæ³¨æ„åŠ›å­å±‚ä¸åŒï¼ŒFFN ç‹¬ç«‹å¤„ç†æ¯ä¸ªä½ç½®ã€‚\n",
    "\n",
    "### è®¡ç®—å…¬å¼ï¼š\n",
    "\n",
    "**a) ä¸¤å±‚çº¿æ€§å˜æ¢ + æ¿€æ´»å‡½æ•°ï¼š**\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "**b) Add & Normï¼š**\n",
    "$$\n",
    "X_3 = \\text{LayerNorm}(X_2 + \\text{Dropout}(\\text{FFN}(X_2)))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. è¾“å‡ºå±‚ï¼ˆçº¿æ€§å˜æ¢ + Softmaxï¼‰\n",
    "\n",
    "Decoder å †å  N å±‚åçš„æœ€ç»ˆè¾“å‡ºå¤„ç†ï¼š\n",
    "\n",
    "### è®¡ç®—å…¬å¼ï¼š\n",
    "\n",
    "**a) çº¿æ€§å˜æ¢åˆ°è¯æ±‡è¡¨å¤§å°ï¼š**\n",
    "$$\n",
    "\\text{Logits} = X_3 W_{\\text{vocab}} + b_{\\text{vocab}}\n",
    "$$\n",
    "å…¶ä¸­ $W_{\\text{vocab}} \\in \\mathbb{R}^{d_{\\text{model}} \\times |V|}$\n",
    "\n",
    "**b) Softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒï¼š**\n",
    "$$\n",
    "P(y_t | y_{<t}, X) = \\text{softmax}(\\text{Logits})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## å®Œæ•´å…¬å¼æ€»ç»“\n",
    "\n",
    "å¯¹äºç¬¬ `l` ä¸ª Decoder Layerï¼š\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X^{(l)}_1 &= \\text{LayerNorm}^{(l)}_1\\left(X^{(l-1)} + \\text{Dropout}\\left(\\text{MaskedSelfAttention}^{(l)}(X^{(l-1)})\\right)\\right) \\\\\n",
    "X^{(l)}_2 &= \\text{LayerNorm}^{(l)}_2\\left(X^{(l)}_1 + \\text{Dropout}\\left(\\text{CrossAttention}^{(l)}(X^{(l)}_1, Z)\\right)\\right) \\\\\n",
    "X^{(l)} &= \\text{LayerNorm}^{(l)}_3\\left(X^{(l)}_2 + \\text{Dropout}\\left(\\text{FFN}^{(l)}(X^{(l)}_2)\\right)\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $X^{(0)} = \\text{Embedding}(Y) + \\text{PE}$ï¼ˆç›®æ ‡åºåˆ—åµŒå…¥+ä½ç½®ç¼–ç ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ æ©ç çŸ©é˜µ $M_t$ çš„æ•°å­¦å®šä¹‰\n",
    "\n",
    "$$\n",
    "M_t(i, j) =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } i \\geq j \\text{ (å…è®¸å…³æ³¨å½“å‰ä½ç½®åŠä¹‹å‰çš„ä½ç½®)} \\\\\n",
    "-\\infty & \\text{if } i < j \\text{ (ç¦æ­¢å…³æ³¨æœªæ¥çš„ä½ç½®)}\n",
    "\\end{cases}\n",
    "$$"
   ],
   "id": "651dd5a57095dac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:30:09.961108Z",
     "start_time": "2025-09-08T03:30:09.955479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,dev_voc_size,max_len,d_model,ffn_hidden,n_head,drop_prod,device,n_layer):\n",
    "        super().__init__()\n",
    "        self.dev_voc_size=dev_voc_size\n",
    "        self.max_len=max_len\n",
    "        self.d_model=d_model\n",
    "        self.ffn_hidden=ffn_hidden\n",
    "        self.n_head=n_head\n",
    "        self.drop_prod=drop_prod\n",
    "        self.device=device\n",
    "        self.n_layer=n_layer\n",
    "        # Embedding\n",
    "        self.embedding=TransformerEmbedding(self.dev_voc_size,self.max_len,self.drop_prod,self.device)\n",
    "        # Layers\n",
    "        self.layers=nn.ModuleList([\n",
    "            DecoderLayer(self.d_model,self.ffn_hidden,self.drop_prod,self.n_head)\n",
    "            for _ in range(self.n_layer)\n",
    "        ])\n",
    "        # å°†ç»“æœæ˜ å°„åˆ°è¯æ±‡è¡¨ä¸Š\n",
    "        self.fc=nn.Linear(in_features=self.d_model,out_features=self.dev_voc_size)\n",
    "    def forward(self,dec,enc,t_mask,s_mask):\n",
    "        dce=self.embedding(enc)\n",
    "        for layer in self.layers:\n",
    "            dec=layer(dec,enc,t_mask,s_mask)\n",
    "        dec=self.fc(dec)\n",
    "        return dec"
   ],
   "id": "86a66c4bd96d958b",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
