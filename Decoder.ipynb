{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-08T03:30:09.446933Z",
     "start_time": "2025-09-08T03:30:07.572966Z"
    }
   },
   "source": [
    "from os import device_encoding\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:30:09.838208Z",
     "start_time": "2025-09-08T03:30:09.831882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 伪代码\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "a6b802e2fd8129be",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:30:09.858077Z",
     "start_time": "2025-09-08T03:30:09.853241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 伪代码\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "c857ca57e6df7116",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:35:38.614710Z",
     "start_time": "2025-09-08T03:35:38.600307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 伪代码\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "f9058eae1e6682c6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 伪代码\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "6ba2caddd27be9c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* 输入向量：$\\mathbf{x}$\n",
    "* Encoder 输出（记作 memory）：$\\mathbf{M}$\n",
    "* 各个子层的结果：依次是 $\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{y}$\n",
    "\n",
    "## DecoderLayer 的公式\n",
    "\n",
    "### 1. Masked Multi-Head Self-Attention\n",
    "\n",
    "对目标序列的输入 $\\mathbf{x}$，做带掩码的多头注意力：\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_1 = \\text{MultiHeadAttention}(\\mathbf{x}, \\mathbf{x}, \\mathbf{x}, \\text{mask})\n",
    "$$\n",
    "\n",
    "加上残差 & LayerNorm：\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_1 = \\text{LayerNorm}(\\mathbf{x} + \\mathbf{z}_1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Encoder–Decoder Attention (Cross-Attention)\n",
    "\n",
    "以 $\\mathbf{x}_1$ 作为 Query，Encoder 的输出 $\\mathbf{M}$ 作为 Key 和 Value：\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_2 = \\text{MultiHeadAttention}(\\mathbf{x}_1, \\mathbf{M}, \\mathbf{M})\n",
    "$$\n",
    "\n",
    "加上残差 & LayerNorm：\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_2 = \\text{LayerNorm}(\\mathbf{x}_1 + \\mathbf{z}_2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Position-wise Feed Forward Network (FFN)\n",
    "\n",
    "FFN 通常是两层前馈网络：\n",
    "\n",
    "$$\n",
    "\\text{FFN}(\\mathbf{x}_2) = \\max(0, \\mathbf{x}_2 \\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2\n",
    "$$\n",
    "\n",
    "加上残差 & LayerNorm：\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x}_2 + \\text{FFN}(\\mathbf{x}_2))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Head Attention 的细节公式\n",
    "\n",
    "每个 **注意力头（head）**：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + \\text{mask}\\right)V\n",
    "$$\n",
    "\n",
    "多头拼接后：\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)\\mathbf{W}^O\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q\\mathbf{W}^Q_i, K\\mathbf{W}^K_i, V\\mathbf{W}^V_i)\n",
    "$$"
   ],
   "id": "781cef7d4db52c73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:30:09.932597Z",
     "start_time": "2025-09-08T03:30:09.924952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,drop_prod,n_head):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.ffn_hidden=ffn_hidden\n",
    "        self.drop_prod=drop_prod\n",
    "        self.n_head=n_head\n",
    "        # Masked Self-Attention\n",
    "        self.self_attention=MultiHeadAttention(self.d_model,self.n_head)\n",
    "        # Cross Attention\n",
    "        self.cross_attention=MultiHeadAttention(self.d_model,self.n_head)\n",
    "        # Position-Wise-Feed-Forward\n",
    "        self.ffn=PositionWiseFeedForward(self.d_model,self.ffn_hidden,self.drop_prod)\n",
    "        # LayerNorm\n",
    "        self.norm1=LayerNorm(self.d_model)\n",
    "        self.norm2=LayerNorm(self.d_model)\n",
    "        self.norm3=LayerNorm(self.d_model)\n",
    "        # Dropout\n",
    "        self.dropout1=nn.Dropout(self.drop_prod)\n",
    "        self.dropout2=nn.Dropout(self.drop_prod)\n",
    "        self.dropout3=nn.Dropout(self.drop_prod)\n",
    "    def forward(self,dec,enc,t_mask,s_mask):\n",
    "        x1=self.norm1(self.dropout1(self.self_attention(dec,dec,dec,t_mask))+dec)\n",
    "        x2=self.norm2(self.dropout2(self.cross_attention(x1,enc,enc,s_mask)+x1))\n",
    "        y=self.norm3(x2+self.dropout3(self.ffn(x2)))\n",
    "        return y"
   ],
   "id": "49ccfe14f8cd6c26",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "好的！以下是 Transformer Decoder 部分的详细计算公式。我们将分步骤解析每个组件的数学表达。\n",
    "\n",
    "---\n",
    "\n",
    "## Decoder 整体计算流程\n",
    "\n",
    "设：\n",
    "- `X`：目标序列的词嵌入（shifted right） + 位置编码，形状为 `(batch_size, tgt_len, d_model)`\n",
    "- `Z`：Encoder 的输出，形状为 `(batch_size, src_len, d_model)`\n",
    "- `M_t`：目标序列的掩码矩阵（防止看到未来信息）\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Masked Multi-Head Self-Attention\n",
    "\n",
    "这是 Decoder 的第一子层，确保自回归性质。\n",
    "\n",
    "### 计算公式：\n",
    "\n",
    "**a) 线性变换得到 Q, K, V：**\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q &= X \\cdot W_Q \\\\\n",
    "K &= X \\cdot W_K \\\\\n",
    "V &= X \\cdot W_V\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**b) Scaled Dot-Product Attention（带掩码）：**\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M_t\\right)V\n",
    "$$\n",
    "\n",
    "**c) Multi-Head 拼接：**\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W_O\n",
    "$$\n",
    "其中 $\\text{head}_i = \\text{Attention}(XW_Q^i, XW_K^i, XW_V^i)$\n",
    "\n",
    "**d) Add & Norm：**\n",
    "$$\n",
    "X_1 = \\text{LayerNorm}(X + \\text{Dropout}(\\text{MultiHead}(X, X, X)))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Multi-Head Cross-Attention\n",
    "\n",
    "这是连接 Encoder 和 Decoder 的关键桥梁。\n",
    "\n",
    "### 计算公式：\n",
    "\n",
    "**a) 线性变换（Q来自Decoder，K,V来自Encoder）：**\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q &= X_1 \\cdot W_Q \\\\\n",
    "K &= Z \\cdot W_K \\\\\n",
    "V &= Z \\cdot W_V\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**b) Scaled Dot-Product Attention：**\n",
    "$$\n",
    "\\text{CrossAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "**c) Multi-Head 拼接：**\n",
    "$$\n",
    "\\text{MultiHeadCross}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W_O\n",
    "$$\n",
    "\n",
    "**d) Add & Norm：**\n",
    "$$\n",
    "X_2 = \\text{LayerNorm}(X_1 + \\text{Dropout}(\\text{MultiHeadCross}(X_1, Z, Z)))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Position-wise Feed-Forward Network\n",
    "\n",
    "与前两个注意力子层不同，FFN 独立处理每个位置。\n",
    "\n",
    "### 计算公式：\n",
    "\n",
    "**a) 两层线性变换 + 激活函数：**\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "**b) Add & Norm：**\n",
    "$$\n",
    "X_3 = \\text{LayerNorm}(X_2 + \\text{Dropout}(\\text{FFN}(X_2)))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 输出层（线性变换 + Softmax）\n",
    "\n",
    "Decoder 堆叠 N 层后的最终输出处理：\n",
    "\n",
    "### 计算公式：\n",
    "\n",
    "**a) 线性变换到词汇表大小：**\n",
    "$$\n",
    "\\text{Logits} = X_3 W_{\\text{vocab}} + b_{\\text{vocab}}\n",
    "$$\n",
    "其中 $W_{\\text{vocab}} \\in \\mathbb{R}^{d_{\\text{model}} \\times |V|}$\n",
    "\n",
    "**b) Softmax 得到概率分布：**\n",
    "$$\n",
    "P(y_t | y_{<t}, X) = \\text{softmax}(\\text{Logits})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 完整公式总结\n",
    "\n",
    "对于第 `l` 个 Decoder Layer：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X^{(l)}_1 &= \\text{LayerNorm}^{(l)}_1\\left(X^{(l-1)} + \\text{Dropout}\\left(\\text{MaskedSelfAttention}^{(l)}(X^{(l-1)})\\right)\\right) \\\\\n",
    "X^{(l)}_2 &= \\text{LayerNorm}^{(l)}_2\\left(X^{(l)}_1 + \\text{Dropout}\\left(\\text{CrossAttention}^{(l)}(X^{(l)}_1, Z)\\right)\\right) \\\\\n",
    "X^{(l)} &= \\text{LayerNorm}^{(l)}_3\\left(X^{(l)}_2 + \\text{Dropout}\\left(\\text{FFN}^{(l)}(X^{(l)}_2)\\right)\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "其中 $X^{(0)} = \\text{Embedding}(Y) + \\text{PE}$（目标序列嵌入+位置编码）\n",
    "\n",
    "---\n",
    "\n",
    "## 掩码矩阵 $M_t$ 的数学定义\n",
    "\n",
    "$$\n",
    "M_t(i, j) =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } i \\geq j \\text{ (允许关注当前位置及之前的位置)} \\\\\n",
    "-\\infty & \\text{if } i < j \\text{ (禁止关注未来的位置)}\n",
    "\\end{cases}\n",
    "$$"
   ],
   "id": "651dd5a57095dac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T03:30:09.961108Z",
     "start_time": "2025-09-08T03:30:09.955479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,dev_voc_size,max_len,d_model,ffn_hidden,n_head,drop_prod,device,n_layer):\n",
    "        super().__init__()\n",
    "        self.dev_voc_size=dev_voc_size\n",
    "        self.max_len=max_len\n",
    "        self.d_model=d_model\n",
    "        self.ffn_hidden=ffn_hidden\n",
    "        self.n_head=n_head\n",
    "        self.drop_prod=drop_prod\n",
    "        self.device=device\n",
    "        self.n_layer=n_layer\n",
    "        # Embedding\n",
    "        self.embedding=TransformerEmbedding(self.dev_voc_size,self.max_len,self.drop_prod,self.device)\n",
    "        # Layers\n",
    "        self.layers=nn.ModuleList([\n",
    "            DecoderLayer(self.d_model,self.ffn_hidden,self.drop_prod,self.n_head)\n",
    "            for _ in range(self.n_layer)\n",
    "        ])\n",
    "        # 将结果映射到词汇表上\n",
    "        self.fc=nn.Linear(in_features=self.d_model,out_features=self.dev_voc_size)\n",
    "    def forward(self,dec,enc,t_mask,s_mask):\n",
    "        dce=self.embedding(enc)\n",
    "        for layer in self.layers:\n",
    "            dec=layer(dec,enc,t_mask,s_mask)\n",
    "        dec=self.fc(dec)\n",
    "        return dec"
   ],
   "id": "86a66c4bd96d958b",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
