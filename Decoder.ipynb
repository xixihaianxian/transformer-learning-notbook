{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-07T15:22:00.883576Z",
     "start_time": "2025-09-07T15:22:00.870063Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 伪代码\n",
    "class MutiHeadAttention(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "a6b802e2fd8129be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 伪代码\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "c857ca57e6df7116"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 伪代码\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()"
   ],
   "id": "f9058eae1e6682c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* 输入向量：$\\mathbf{x}$\n",
    "* Encoder 输出（记作 memory）：$\\mathbf{M}$\n",
    "* 各个子层的结果：依次是 $\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{y}$\n",
    "\n",
    "## DecoderLayer 的公式\n",
    "\n",
    "### 1. Masked Multi-Head Self-Attention\n",
    "\n",
    "对目标序列的输入 $\\mathbf{x}$，做带掩码的多头注意力：\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_1 = \\text{MultiHeadAttention}(\\mathbf{x}, \\mathbf{x}, \\mathbf{x}, \\text{mask})\n",
    "$$\n",
    "\n",
    "加上残差 & LayerNorm：\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_1 = \\text{LayerNorm}(\\mathbf{x} + \\mathbf{z}_1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Encoder–Decoder Attention (Cross-Attention)\n",
    "\n",
    "以 $\\mathbf{x}_1$ 作为 Query，Encoder 的输出 $\\mathbf{M}$ 作为 Key 和 Value：\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_2 = \\text{MultiHeadAttention}(\\mathbf{x}_1, \\mathbf{M}, \\mathbf{M})\n",
    "$$\n",
    "\n",
    "加上残差 & LayerNorm：\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_2 = \\text{LayerNorm}(\\mathbf{x}_1 + \\mathbf{z}_2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Position-wise Feed Forward Network (FFN)\n",
    "\n",
    "FFN 通常是两层前馈网络：\n",
    "\n",
    "$$\n",
    "\\text{FFN}(\\mathbf{x}_2) = \\max(0, \\mathbf{x}_2 \\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2\n",
    "$$\n",
    "\n",
    "加上残差 & LayerNorm：\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x}_2 + \\text{FFN}(\\mathbf{x}_2))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Head Attention 的细节公式\n",
    "\n",
    "每个 **注意力头（head）**：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + \\text{mask}\\right)V\n",
    "$$\n",
    "\n",
    "多头拼接后：\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)\\mathbf{W}^O\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q\\mathbf{W}^Q_i, K\\mathbf{W}^K_i, V\\mathbf{W}^V_i)\n",
    "$$"
   ],
   "id": "781cef7d4db52c73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,ffn_hidden,drop_prod):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.ffn_hidden=ffn_hidden\n",
    "        self.drop_prod=drop_prod\n",
    "        self.attention=MutiHeadAttention(self.d_model,self.n_head)\n",
    "        self.norm=LayerNorm(self.d_model)\n",
    "        self.dropout=nn.Dropout(self.drop_prod)\n",
    "        self.ffn=PositionWiseFeedForward(self.d_model,self.ffn_hidden,self.drop_prod)\n",
    "    def forward(self,dec,enc,t_mask,s_mask):\n"
   ],
   "id": "49ccfe14f8cd6c26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
