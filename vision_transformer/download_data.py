import requests
import os
from loguru import logger
from pathlib import Path
from tqdm import tqdm
import zipfile
from typing import List, Tuple, Union, Any
import config
from urllib.parse import urljoin
import time
import threading
import random

def recursive_unzip(zip_name:str,unpack_path:str):
    r"""
    :param zip_name: å‹ç¼©åŒ…åç§°
    :param unpack_path: è§£å‹ç›®æ ‡è·¯å¾„
    :return: None
    """
    # åˆ¤æ–­ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(unpack_path):
        # raise FileExistsError(f"{unpack_path} not is exists.")
        os.makedirs(unpack_path,exist_ok=True)
    # åˆ¤æ–­å‹ç¼©åŒ…æ˜¯å¦å­˜åœ¨
    if not os.path.exists(zip_name):
        raise FileExistsError(f"{zip_name} not is exists.")
    # å°†æ ¹ç›®å½•è§£å‹
    with zipfile.ZipFile(zip_name,"r") as zip_ref:
        zip_ref.extractall(unpack_path)
    # å¯¹è§£è¯å‹ä¹‹åçš„ç›®å½•è¿›è¡Œéå†
    for root,dirs,files in os.walk(unpack_path):
        for file in files:
            next_zip_name=os.path.join(root,file)
            if zipfile.is_zipfile(next_zip_name):
                next_unpack_path=os.path.splitext(next_zip_name)[0]
                recursive_unzip(next_zip_name,next_unpack_path)
    # è§£å‹æˆåŠŸæ—¥å¿—
    logger.info(f"unpack successful!")

def download_data(url:str,name:str=None,data_path:str=None,need_unpack:bool=True):
    r"""
    Download the file from the given file path to the specified path and unzip it.
    :param url: download url
    :param name: zip name
    :param data_path: zip root dir(å‹ç¼©åŒ…æ ¹è·¯å¾„)
    :param need_unpack: Is it necessary to extract?
    :return:
    """
    # åˆ¤æ–­æ˜¯nameæ˜¯å¦æå‰å®šä¹‰
    if name is not None:
        pass
    else:
        name=os.path.basename(url) # è·å–å‹ç¼©åŒ…çš„åç§°ï¼ŒåŒ…å«å‹ç¼©åŒ…ç±»å‹
    # åˆ¤æ–­data_pathæ˜¯å¦å®šä¹‰
    if data_path is None:
        data_path="data"
    # åˆ›å»ºæ–‡ä»¶å¤¹
    if not os.path.exists(data_path):
        logger.info(f"Create {os.path.join(os.getcwd(),data_path)}")
    data_dir=os.path.join(os.getcwd(),data_path)
    os.makedirs(data_dir,exist_ok=True)
    # è®¾ç½®ä»£ç†
    proxies = {
        "http": "http://127.0.0.1:7890", # è¿™é‡Œæ›¿æ¢æˆä½ ä»£ç†çš„ç«¯å£å·
        "https": "http://127.0.0.1:7890",
    }
    zip_path=os.path.join(data_dir,name)
    # è·å–æ•°æ®
    response = requests.get(url, stream=True, proxies=proxies)
    # è·å–æ•°æ®é•¿åº¦
    file_size = int(response.headers.get("Content-Length"))
    # ä¸‹è½½æ•°æ®åˆ°data_path
    with open(zip_path,"wb") as f,tqdm(
        total=file_size,
        unit="B",
        unit_divisor=1024,
        unit_scale=True,
    ) as bar: # è®¾ç½®è¿›åº¦æ¡
        logger.info(f"Downloading {data_path}")
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                f.write(chunk)
                bar.update(1024)
    # å‹ç¼©ç±»å‹
    compression_formats = [
        "zip",
        "7z",
        "rar",
        "tar",
        "tgz",
        "tbz2",
    ]
    # è§£å‹æ•°æ®
    if need_unpack: # åˆ¤æ–­æ˜¯å¦éœ€è¦è§£å‹å‹ç¼©åŒ…
        try:
            compress_type=name.split(sep=".")[1] # è·å–å‹ç¼©ç±»å‹
            zip_name=name.split(sep=".")[0] # è·å–è¦è§£å‹åˆ°æ–‡ä»¶çš„åç§°
            unpack_path=os.path.join(data_dir,zip_name) # è§£å‹åˆ°æ–‡ä»¶çš„è·¯å¾„
            if compress_type in compression_formats:
                recursive_unzip(zip_path,unpack_path)
            else:
                raise ValueError
        except IndexError as e:
            logger.warning(f"Not find compress file.")
        except ValueError as e:
            logger.warning(f"Not find compress type.")

# GithubåŠ é€Ÿç»„ä»¶
class GitHubAccelerator:
    def __init__(self):
        # å®šä¹‰æ™®é€šä¸‹è½½é“¾æ¥
        self.download_url=config.DOWNLOAD_URL
        # å®šä¹‰rawä¸‹è½½é“¾æ¥
        self.raw_url=config.RAW_URL
    # é‡å†™githubä¸‹è½½çš„url
    def rewrite_github_url(self,original_url:str,node_type:str="download")->List[Tuple[str,str,str]]:
        # åˆ¤æ–­é“¾æ¥æ˜¯å¦æ˜¯githubå†…éƒ¨çš„é“¾æ¥
        accelerated_urls = []
        if "github.com" not in original_url:
            raise ValueError(f"{original_url} not is a github url!")
        #åˆ¤æ–­ä¸‹è½½ç±»å‹
        nodes=self.download_url if node_type=="download" else self.raw_url
        # æ„å»ºæé€Ÿä¸‹è½½çš„æ¸…å•
        for node_url,countries,description in nodes:
            # å¤„ç†ä¸‹è½½é“¾æ¥
            if node_type=="download":
                accelerated_url=original_url.replace(
                    "https://github.com",
                    node_url
                )
            # å¤„ç†rawé“¾æ¥
            elif node_type=="raw":
                if "/blob/" in original_url:
                    # accelerated_url=original_url.replace(
                    #     "https://github.com",
                    #     node_url
                    # ).replace("/blob/","/")
                    accelerated_url=urljoin(node_url,original_url)
                else:
                    accelerated_url=original_url.replace(
                        "https://github.com",
                        node_url
                    )
            else:
                accelerated_url=original_url
            accelerated_urls.append((accelerated_url,countries,description))
        return accelerated_urls
    # æµ‹é€Ÿ
    def test_node_speed(self,url:str,timeout=5):
        logger.info(f"ğŸ” æ­£åœ¨æµ‹è¯•:{url}")
        try:
            start_time=time.time()
            # å®šä¹‰è¯·æ±‚å¤´,æ ¹æ®è‡ªå·±çš„ç”µè„‘æ¥å®šä¹‰
            headers = {'User-Agent': 'Mozilla/5.0 (GitHub Accelerator Speed Test)'}
            response=requests.head(url=url,timeout=timeout,allow_redirects=True,headers=headers)
            if response.status_code==200:
                return time.time()-start_time
        # æµ‹è¯•å¤±è´¥è¿”å›None,è¡¨ç¤ºè¶…æ—¶
        except Exception as e:
            return None
    # ä½¿ç”¨å»¶è¿Ÿæœ€å°çš„urlæ¥ä¸‹è½½æ–‡ä»¶
    def get_fastest_node(self,accelerated_urls:List[Tuple[str,str,str]])-> List[str]:
        results=[]
        threads=[]
        # è·å–å¯ç”¨çš„url
        # for accelerated_url,countries,description in accelerated_urls:
        #     speed=self.test_node_speed(accelerated_url)
        #     if speed is not None:
        #         results.append((accelerated_url,countries,description,speed))
        def test_speed_thread(url_desc):
            accelerated_url, countries, description=url_desc
            speed = self.test_node_speed(accelerated_url)
            if speed is not None:
                results.append((accelerated_url, countries, description, speed))
        # å¤šçº¿ç¨‹åŠ é€Ÿç½‘ç«™çš„æ£€æµ‹
        for url_desc in accelerated_urls:
            thread=threading.Thread(target=test_speed_thread,kwargs={"url_desc":url_desc})
            threads.append(thread)
            thread.start()
        for thread in threads:
            thread.join(timeout=10)
        # è·å–fastest node
        if results:
            results.sort(key=lambda x:x[3])
            results=list(map(lambda t:(t[0],t[1],t[2]),results))
            return results[0]
        else:
            return random.choice(accelerated_urls)
    # ä¸‹è½½
    def download_file(self,url_desc):
        accelerated_url, countries, description= url_desc
        logger.info(f"ğŸ“ æ­£åœ¨ä½¿ç”¨{countries}çš„èŠ‚ç‚¹ã€‚")
        logger.info(f"ğŸ™ æ„Ÿè°¢{description}ã€‚")
        download_data(url=accelerated_url)

if __name__=="__main__":
    original_url="https://github.com/PowerShell/PowerShell/releases/download/v7.5.3/powershell-7.5.3-linux-arm64.tar.gz"
    gitHub_accelerator=GitHubAccelerator()
    urls=gitHub_accelerator.rewrite_github_url(original_url)
    url_desc=gitHub_accelerator.get_fastest_node(urls)
    gitHub_accelerator.download_file(url_desc)