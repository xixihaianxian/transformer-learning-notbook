import requests
import os
from loguru import logger
from pathlib import Path
from tqdm import tqdm
import zipfile
import tarfile
import rarfile
from typing import List, Tuple, Union, Any
import config
from urllib.parse import urljoin
import time
import threading
import random
import re
import subprocess
from urllib import parse
import argparse

# è§£å‹zipæ¨¡å—
def recursive_unzip(zip_name:str,unpack_path:str):
    r"""
    :param zip_name: å‹ç¼©åŒ…åç§°
    :param unpack_path: è§£å‹ç›®æ ‡è·¯å¾„
    :return: None
    """
    # åˆ¤æ–­ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(unpack_path):
        # raise FileExistsError(f"{unpack_path} not is exists.")
        os.makedirs(unpack_path,exist_ok=True)
    # åˆ¤æ–­å‹ç¼©åŒ…æ˜¯å¦å­˜åœ¨
    if not os.path.exists(zip_name):
        raise FileExistsError(f"{zip_name} not is exists.")
    # å°†æ ¹ç›®å½•è§£å‹
    # zipå‹ç¼©åŒ…çš„è§£å‹
    if zipfile.is_zipfile(zip_name):
        with zipfile.ZipFile(zip_name,"r") as zip_ref:
            zip_ref.extractall(unpack_path)
    # tarå‹ç¼©åŒ…è§£å‹
    elif tarfile.is_tarfile(zip_name):
        with tarfile.open(zip_name,mode="r:*") as tar_ref:
            tar_ref.extractall(unpack_path)
    # rarå‹ç¼©åŒ…è§£å‹
    elif rarfile.is_rarfile(zip_name):
        with rarfile.RarFile(zip_name, "r") as rar_ref:
            rar_ref.extractall(unpack_path)
    # else:
    #     logger.error(f"âŒ Unable to parse this type of compressed file.") # æ— æ³•è§£æè¿™ç±»å‹ç¼©åŒ…ï¼Œé”™è¯¯æ—¥å¿—
    #     return
    # å¯¹è§£å‹ä¹‹åçš„ç›®å½•è¿›è¡Œéå†
    for root,dirs,files in os.walk(unpack_path):
        for file in files:
            next_zip_name=os.path.join(root,file)
            # åˆ¤æ–­äºŒçº§æ–‡ä»¶æ˜¯ä¸æ˜¯å‹ç¼©åŒ…
            if zipfile.is_zipfile(next_zip_name) or tarfile.is_tarfile(next_zip_name) or rarfile.is_rarfile(next_zip_name):
                next_unpack_path=os.path.splitext(next_zip_name)[0]
                recursive_unzip(next_zip_name,next_unpack_path)
    # è§£å‹æˆåŠŸæ—¥å¿—
    logger.info(f"âœ… unpack successful!")

# ä¸‹è½½
def download_data(url:str,name:str=None,data_path:str=None,need_unpack:bool=True):
    r"""
    Download the file from the given file path to the specified path and unzip it.
    It is recommended to set the name.
    :param url: download url
    :param name: zip name (åŒ…å«åç¼€.zip)
    :param data_path: zip root dir(å‹ç¼©åŒ…æ ¹è·¯å¾„)
    :param need_unpack: Is it necessary to extract?(æ˜¯å¦éœ€è¦è§£å‹)
    :return: None
    """
    # åˆ¤æ–­æ˜¯nameæ˜¯å¦æå‰å®šä¹‰
    if name is not None:
        pass
    else:
        name=os.path.basename(url) # è·å–å‹ç¼©åŒ…çš„åç§°ï¼ŒåŒ…å«å‹ç¼©åŒ…ç±»å‹
    # åˆ¤æ–­data_pathæ˜¯å¦å®šä¹‰
    if data_path is None:
        data_path="data"
    # åˆ›å»ºæ–‡ä»¶å¤¹
    if not os.path.exists(data_path):
        logger.info(f"Create {os.path.join(os.getcwd(),data_path)}")
    data_dir=os.path.join(os.getcwd(),data_path)
    os.makedirs(data_dir,exist_ok=True)
    # è®¾ç½®ä»£ç†
    proxies = {
        "http": "http://127.0.0.1:7890", # è¿™é‡Œæ›¿æ¢æˆä½ ä»£ç†çš„ç«¯å£å·
        "https": "http://127.0.0.1:7890",
    }
    zip_path=os.path.join(data_dir,name)
    # è·å–æ•°æ®
    response = requests.get(url, stream=True, proxies=proxies)
    # è·å–æ•°æ®é•¿åº¦
    file_size = int(response.headers.get("Content-Length"))
    # è·å–æ–‡ä»¶ç±»å‹
    file_type=str(response.headers.get("Content-Type"))
    # ä¸‹è½½æ•°æ®åˆ°data_path
    with open(zip_path,"wb") as f,tqdm(
        total=file_size,
        unit="B",
        unit_divisor=1024,
        unit_scale=True,
    ) as bar: # è®¾ç½®è¿›åº¦æ¡
        logger.info(f"Downloading {data_path}")
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                f.write(chunk)
                bar.update(1024)
    # å‹ç¼©ç±»å‹
    compression_formats = [
        "zip",
        "rar",
        "tar.gz",
        "gz"
    ]
    # è§£å‹æ•°æ®
    if need_unpack: # åˆ¤æ–­æ˜¯å¦éœ€è¦è§£å‹å‹ç¼©åŒ…
        try:
            # compress_type=name.split(sep=".")[1] # è·å–å‹ç¼©ç±»å‹
            zip_name=name.split(sep=".")[0] # è·å–è¦è§£å‹åˆ°æ–‡ä»¶çš„åç§°
            unpack_path=os.path.join(data_dir,zip_name) # è§£å‹åˆ°æ–‡ä»¶çš„è·¯å¾„
            recursive_unzip(zip_path, unpack_path)
            # if compress_type in compression_formats:
            #     recursive_unzip(zip_path,unpack_path)
            # else:
            #     raise ValueError
        except IndexError as e:
            logger.warning(f"Not find compress file.")
        # except ValueError as e:
        #     logger.warning(f"Not find compress type.")

# GithubåŠ é€Ÿç»„ä»¶
class GitHubAccelerator:
    def __init__(self):
        # å®šä¹‰æ™®é€šä¸‹è½½é“¾æ¥
        self.download_url=config.DOWNLOAD_URL
        # å®šä¹‰rawä¸‹è½½é“¾æ¥
        self.raw_url=config.RAW_URL
        # å®šä¹‰clone urlä¸‹è½½é“¾æ¥
        self.clone_url=config.CLONE_URL
    # é‡å†™githubä¸‹è½½çš„url
    def rewrite_github_url(self,original_url:str,node_type:str="download")->List[Tuple[str,str,str]]:
        # åˆ¤æ–­é“¾æ¥æ˜¯å¦æ˜¯githubå†…éƒ¨çš„é“¾æ¥
        accelerated_urls = []
        if "github.com" not in original_url:
            raise ValueError(f"{original_url} not is a github url!")
        #åˆ¤æ–­ä¸‹è½½ç±»å‹
        nodes=self.download_url if node_type=="download" else self.raw_url
        # if node_type=="download":
        #     nodes=self.download_url
        # elif node_type=="raw":
        #     nodes=self.raw_url
        # elif node_type=="clone":
        #     nodes=self.clone_url
        # else:
        #     nodes=self.download_url # é»˜è®¤ç­‰äºdownload_url
        # æ„å»ºæé€Ÿä¸‹è½½çš„æ¸…å•
        for node_url,countries,description in nodes:
            # å¤„ç†ä¸‹è½½é“¾æ¥
            if node_type=="download":
                accelerated_url=original_url.replace(
                    "https://github.com",
                    node_url
                )
            # å¤„ç†rawé“¾æ¥
            elif node_type=="raw":
                if "/blob/" in original_url:
                    accelerated_url=original_url.replace(
                        "https://github.com",
                        node_url
                    ).replace("/blob/","/")
                    # accelerated_url=urljoin(node_url,original_url)
                else:
                    accelerated_url=original_url.replace(
                        "https://github.com",
                        node_url
                    )
            else:
                accelerated_url=original_url
            accelerated_urls.append((accelerated_url,countries,description))
        return accelerated_urls
    # é‡å†™clone url:
    def rewrite_clone_url(self,original_url:str)->List[Tuple[str,str,str]]:
        stype=os.path.splitext(original_url)[1]
        accelerated_urls = []
        # åˆ¤æ–­æ˜¯å¦æ˜¯clone urlè·¯å¾„
        if stype!=".git":
            raise ValueError(f"{original_url} not is a clone url!")
        else:
            href_split = original_url.split("github.com", 1)[1]
            for node_url,countries,description in self.clone_url:
                if node_url.startswith("https://gitclone.com"):
                    accelerated_url=node_url+"/github.co"+href_split
                else:
                    accelerated_url=node_url+href_split
                accelerated_urls.append((accelerated_url,countries,description))
        return accelerated_urls
    # æµ‹é€Ÿ
    def test_node_speed(self,url:str,timeout=5):
        logger.info(f"ğŸ” æ­£åœ¨æµ‹è¯•:{url}")
        try:
            start_time=time.time()
            # å®šä¹‰è¯·æ±‚å¤´,æ ¹æ®è‡ªå·±çš„ç”µè„‘æ¥å®šä¹‰
            headers = {'User-Agent': 'Mozilla/5.0 (GitHub Accelerator Speed Test)'}
            response=requests.head(url=url,timeout=timeout,allow_redirects=True,headers=headers)
            if response.status_code==200:
                return time.time()-start_time
        # æµ‹è¯•å¤±è´¥è¿”å›None,è¡¨ç¤ºè¶…æ—¶
        except Exception as e:
            return None
    # ä½¿ç”¨å»¶è¿Ÿæœ€å°çš„urlæ¥ä¸‹è½½æ–‡ä»¶
    def get_fastest_node(self,accelerated_urls:List[Tuple[str,str,str]])-> List[str]:
        results=[]
        threads=[]
        # è·å–å¯ç”¨çš„url
        # for accelerated_url,countries,description in accelerated_urls:
        #     speed=self.test_node_speed(accelerated_url)
        #     if speed is not None:
        #         results.append((accelerated_url,countries,description,speed))
        def test_speed_thread(url_desc):
            accelerated_url, countries, description=url_desc
            speed = self.test_node_speed(accelerated_url)
            if speed is not None:
                results.append((accelerated_url, countries, description, speed))
        # å¤šçº¿ç¨‹åŠ é€Ÿç½‘ç«™çš„æ£€æµ‹
        for url_desc in accelerated_urls:
            thread=threading.Thread(target=test_speed_thread,kwargs={"url_desc":url_desc})
            threads.append(thread)
            thread.start()
        for thread in threads:
            thread.join(timeout=10)
        # è·å–fastest node
        if results:
            results.sort(key=lambda x:x[3])
            results=list(map(lambda t:(t[0],t[1],t[2]),results))
            return results[0]
        else:
            return random.choice(accelerated_urls)
    # ä¸‹è½½
    def download_file(self,url_desc,name:str=None,data_path:str=None,need_unpack:bool=True):
        accelerated_url, countries, description= url_desc
        logger.info(f"ğŸ“ æ­£åœ¨ä½¿ç”¨{countries}çš„èŠ‚ç‚¹ã€‚")
        logger.info(f"ğŸ™ æ„Ÿè°¢{description}ã€‚")
        # ä¸‹è½½æ•°æ®
        download_data(url=accelerated_url,name=name,data_path=data_path,need_unpack=need_unpack)
    # clone Project,è¿è¡Œä¹‹å‰è¯·ç¡®è®¤ä½ å·²ç»å®‰è£…äº†git
    def clone_project(self,accelerated_urls:List[Tuple[str,str,str]],target_directory:str=None):
        for accelerated_url, countries, description in accelerated_urls:
            if target_directory is None:
                target_directory=os.path.basename(accelerated_url).split(".")[0]
            logger.info(f"ğŸ“ æ­£åœ¨ä½¿ç”¨{countries}çš„èŠ‚ç‚¹ã€‚")
            logger.info(f"ğŸ™ æ„Ÿè°¢{description}ã€‚")
            try:
                # åˆ¤æ–­æ˜¯å¦æŒ‡å®šcloneåˆ°æŒ‡å®šç›®å½•
                # if target_directory is None:
                #     subprocess.run(["git","clone",accelerated_url],timeout=300,check=True)
                # else:
                subprocess.run(["git", "clone", accelerated_url,target_directory], timeout=300, check=True)
                logger.info(f"âœ… clone project successful!")
                break
            except subprocess.TimeoutExpired as e:
                # åˆ é™¤é¡¹ç›®æ–‡ä»¶ï¼Œé˜²æ­¢ä¸‹ä¸€æ¬¡cloneæ—¶å‡ºé”™
                if os.path.exists(target_directory):
                    os.removedirs(target_directory)
                logger.error("â° time out!")
            except subprocess.CalledProcessError as e:
                # åˆ é™¤é¡¹ç›®æ–‡ä»¶ï¼Œé˜²æ­¢ä¸‹ä¸€æ¬¡cloneæ—¶å‡ºé”™
                if os.path.exists(target_directory):
                    os.removedirs(target_directory)
                logger.error(f"âŒ git å‘½ä»¤å¤±è´¥:{e}")
# æ·»åŠ æŒ‡ä»¤æ“ä½œ
def main():
    parser=argparse.ArgumentParser(description="ğŸ™‚ Github accelerate tool")
    parser.add_argument("--clone",dest="clone",action="store_true") # è®¾ç½®æ˜¯å¦æ˜¯clone url
    parser.add_argument("-u","--url",type=str,help="clone url/download url/raw url",dest="url") # è®¾ç½® url
    parser.add_argument("-t","--type",type=str,dest="type",choices=["download","raw"],default="download",help="download type") # ä¸‹è½½ç±»å‹
    parser.add_argument("--zip_name","-z",type=str,dest="zip_name",help="zip name(Including the suffix)") # è®¾ç½®å‹ç¼©åç§°
    parser.add_argument("--data_path","-dp",dest="data_path",help="The path to save the compressed file") # è®¾ç½®å‹ç¼©åŒ…å­˜æ”¾çš„è·¯å¾„
    parser.add_argument("--unzip",dest="unzip",action="store_true",help="Determine whether to extract") # åˆ¤æ–­æ˜¯å¦è¦è§£å‹
    args=parser.parse_args()
    # ç¼–å†™é€»è¾‘
    # original_url="https://github.com/PowerShell/PowerShell.git"
    gitHub_accelerator = GitHubAccelerator()
    if args.clone:
        logger.info(f"ğŸ“¦ clone {args.url}") # æç¤ºè¦å…‹éš†çš„é¡¹ç›®
        urls = gitHub_accelerator.rewrite_clone_url(args.url)
        gitHub_accelerator.clone_project(accelerated_urls=urls)
    else:
        urls = gitHub_accelerator.rewrite_github_url(original_url=args.url,node_type=args.type)
        url_desc = gitHub_accelerator.get_fastest_node(urls)
        gitHub_accelerator.download_file(url_desc,name=args.zip_name,data_path=args.data_path,need_unpack=args.unzip)
if __name__=="__main__":
    main()