{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### vision transformer for pytorch",
   "id": "d8c6dcfe19eb3bf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:47:08.904746Z",
     "start_time": "2025-09-18T02:47:05.503677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn"
   ],
   "id": "e179775f2ad20f24",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. **PatchEmbed çš„åŠ¨æœº**\n",
    "\n",
    "åœ¨ **NLP** é‡Œï¼Œè¾“å…¥æ˜¯å•è¯åºåˆ—ï¼Œæ¯ä¸ªå•è¯ä¼šé€šè¿‡ **embedding** è½¬æ¢æˆå‘é‡ï¼Œè¾“å…¥ç»™ Transformerã€‚\n",
    "ä½†æ˜¯ **å›¾åƒ**æ˜¯äºŒç»´åƒç´ çŸ©é˜µï¼ˆæ¯”å¦‚ `224Ã—224Ã—3`ï¼‰ï¼Œä¸èƒ½ç›´æ¥è¾“å…¥åˆ° Transformerã€‚\n",
    "\n",
    "äºæ˜¯ ViT æå‡ºäº†ä¸€ä¸ªåŠæ³•ï¼š\n",
    "ğŸ‘‰ æŠŠå›¾åƒåˆ‡åˆ†æˆå¾ˆå¤šå°å—ï¼ˆpatchï¼‰ï¼Œå†æŠŠæ¯ä¸ª patch å½“ä½œä¸€ä¸ªâ€œå•è¯â€ï¼ŒæŠ•å½±åˆ°é«˜ç»´ç©ºé—´ï¼Œå¾—åˆ° patch embeddingã€‚è¿™æ ·å›¾åƒå°±è¢«è½¬æ¢æˆä¸€ä¸ªåºåˆ—ï¼Œèƒ½è¾“å…¥ Transformerã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **PatchEmbed çš„ä½œç”¨**\n",
    "\n",
    "`PatchEmbed` çš„ä¸»è¦åŠŸèƒ½æ˜¯ï¼š\n",
    "\n",
    "1. **åˆ‡åˆ†å›¾åƒ**\n",
    "\n",
    "   * è¾“å…¥å›¾åƒå¤§å°ï¼š`img_size Ã— img_size Ã— in_c`\n",
    "   * åˆ’åˆ†æˆ `patch_size Ã— patch_size` çš„å°å—\n",
    "   * å¾—åˆ° `num_patches = (img_size / patch_size)Â²` ä¸ª patch\n",
    "\n",
    "2. **å±•å¹³ patch**\n",
    "\n",
    "   * æ¯ä¸ª patch å±•å¹³æˆä¸€ç»´å‘é‡ã€‚\n",
    "   * åŸå§‹ç»´åº¦ï¼š`patch_size Ã— patch_size Ã— in_c`\n",
    "\n",
    "3. **çº¿æ€§æ˜ å°„åˆ° embed\\_dim**\n",
    "\n",
    "   * ç”¨ `Linear` æˆ–ç­‰æ•ˆçš„ `Conv2d` åšæŠ•å½±\n",
    "   * æ¯ä¸ª patch æœ€ç»ˆå˜æˆ `embed_dim` ç»´åº¦çš„å‘é‡\n",
    "\n",
    "4. **ï¼ˆå¯é€‰ï¼‰å½’ä¸€åŒ–**\n",
    "\n",
    "   * ä¸€èˆ¬åœ¨ embedding åé¢åŠ  `LayerNorm`ï¼Œå¸®åŠ©è®­ç»ƒç¨³å®š\n",
    "\n",
    "æœ€ç»ˆè¾“å‡ºï¼š\n",
    "\n",
    "* ä¸€ä¸ªå½¢çŠ¶ä¸º `[B, num_patches, embed_dim]` çš„ patch embedding åºåˆ—ï¼Œèƒ½ç›´æ¥è¾“å…¥ Transformerã€‚"
   ],
   "id": "d0518599c3d35c1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self,img_size=224,patch_size=16,in_c=3,embed_dim=768,norm_layer=None):\n",
    "        # img_size å›¾åƒå¤§å°\n",
    "        # patch_size æ¯ä¸ªpatchçš„å¤§å°\n",
    "        # embed_dim åµŒå…¥çš„ç»´åº¦\n",
    "        super().__init__()\n",
    "        img_size=(img_size,img_size) # å°†è¾“å…¥çš„å›¾åƒå¤§å°å˜ä¸ºä¸€ä¸ªäºŒç»´å…ƒç»„\n",
    "        patch_size=(patch_size,patch_size)\n",
    "        self.img_size=img_size\n",
    "        self.patch_size=patch_size\n",
    "        self.gird_size=(img_size[0]//patch_size[0],img_size[1]//patch_size[1]) # patchçš„ç½‘æ ¼å¤§å°\n",
    "        self.num_patches=self.gird_size[0]*self.gird_size[1] # patchçš„æ•°é‡\n",
    "        self.proj=nn.Conv2d(in_c,embed_dim,kernel_size=patch_size,stride=patch_size)\n",
    "        self.norm=norm_layer(embed_dim) if norm_layer else nn.Identity() # è‹¥æœ‰layer_normåˆ™ä½¿ç”¨ï¼Œè‹¥æ— åˆ™ä¿æŒä¸å˜\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        B,C,H,W=x.shape # B:batch_size C:channel H:height W:width\n",
    "        assert H==self.img_size[0] and W==self.img_size[1],f\"è¾“å…¥å›¾åƒçš„å¤§å°ä¸æ¨¡å‹æœŸæœ›å¤§å°{self.img_size[0]*self.img_size[1]}ä¸åŒ¹é…\"\n",
    "        x=self.proj(x).flatten(2).transpose(1,2)\n",
    "        x=self.norm(x) # è‹¥æœ‰å½’ä¸€åŒ–å±‚åˆ™ä½¿ç”¨\n",
    "        return x"
   ],
   "id": "599717f68401e78d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. **èƒŒæ™¯**\n",
    "\n",
    "åœ¨ ViT é‡Œï¼Œå›¾åƒè¢«åˆ‡åˆ†æˆ patchï¼ˆæ¯”å¦‚ 196 ä¸ª patchï¼‰ï¼Œæ¯ä¸ª patch éƒ½è¢«åµŒå…¥æˆä¸€ä¸ªå‘é‡ã€‚\n",
    "Transformer çš„æ ¸å¿ƒå°±æ˜¯ **Attention æœºåˆ¶**ï¼Œå®ƒèƒ½è®©è¿™äº› patch ä¹‹é—´å»ºç«‹è”ç³»ï¼š\n",
    "\n",
    "* å“ªäº› patch æ›´ç›¸å…³\n",
    "* å¦‚ä½•æŠŠå…¨å±€ä¿¡æ¯èåˆ\n",
    "\n",
    "åœ¨ ViT ä¸­ï¼Œä¸»è¦ä½¿ç”¨çš„æ˜¯ **å¤šå¤´è‡ªæ³¨æ„åŠ› (Multi-Head Self-Attention, MHSA)**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Attention çš„åŸºæœ¬æ€æƒ³**\n",
    "\n",
    "å¯¹äºè¾“å…¥åºåˆ— \\$X \\in \\mathbb{R}^{N \\times d}\\$ ï¼ˆ\\$N\\$ = patch æ•°ï¼Œ\\$d\\$ = embedding ç»´åº¦ï¼‰ï¼š\n",
    "Attention ä¼šå­¦ä¹ ä¸åŒ patch ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚\n",
    "\n",
    "**æ­¥éª¤ï¼š**\n",
    "\n",
    "1. **æŠ•å½±æˆ Q, K, V**\n",
    "\n",
    "   * \\$Q = XW\\_Q,\\ K = XW\\_K,\\ V = XW\\_V\\$\n",
    "   * å…¶ä¸­ \\$W\\_Q, W\\_K, W\\_V \\in \\mathbb{R}^{d \\times d\\_h}\\$ æ˜¯å¯å­¦ä¹ å‚æ•°\n",
    "\n",
    "2. **è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**\n",
    "\n",
    "   * \\$A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d\\_h}}\\right)\\$\n",
    "   * è¡¨ç¤ºæ¯ä¸ª patch å¯¹å…¶ä»– patch çš„å…³æ³¨ç¨‹åº¦\n",
    "\n",
    "3. **åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º**\n",
    "\n",
    "   * \\$O = AV\\$\n",
    "   * æ¯ä¸ª patch çš„æ–°è¡¨ç¤ºæ˜¯æ‰€æœ‰ patch ä¿¡æ¯çš„åŠ æƒå’Œ\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **å¤šå¤´æœºåˆ¶ (Multi-Head)**\n",
    "\n",
    "å•å¤´å¯èƒ½åªå­¦åˆ°ä¸€ç§å…³ç³»ï¼Œæ‰€ä»¥ ViT ä½¿ç”¨ **å¤šå¤´ (Multi-Head)**ï¼š\n",
    "\n",
    "* è¾“å…¥å‘é‡è¢«åˆ†æˆå¤šä¸ªå­ç©ºé—´ï¼ˆheadï¼‰\n",
    "* æ¯ä¸ª head å•ç‹¬åš Attention\n",
    "* æœ€åæ‹¼æ¥èµ·æ¥ï¼Œå†é€šè¿‡çº¿æ€§å±‚èåˆ\n",
    "\n",
    "å…¬å¼ï¼š\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\n",
    "$$"
   ],
   "id": "d04c0057ee04760b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. **ä¸ºä»€ä¹ˆä¸æ˜¯ N = num\\_patchï¼Œè€Œæ˜¯ num\\_patch + 1ï¼Ÿ**\n",
    "\n",
    "åœ¨ ViT é‡Œï¼Œé™¤äº† patch embedding å¤–ï¼Œè¿˜ä¼šé¢å¤–å¼•å…¥ä¸€ä¸ª **class token**ï¼Œæ‰€ä»¥æ€»é•¿åº¦æ¯” patch æ•°å¤š 1ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **class token çš„ä½œç”¨**\n",
    "\n",
    "* ç±»ä¼¼äº NLP é‡Œ BERT çš„ `[CLS]` tokenã€‚\n",
    "* å®ƒæ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‘é‡ï¼ˆå‚æ•°ï¼‰ï¼Œåœ¨åºåˆ—æœ€å‰é¢æ‹¼æ¥åˆ°æ‰€æœ‰ patch embedding å‰é¢ï¼š\n",
    "\n",
    "$$\n",
    "X = [x_{cls}, x_1, x_2, \\dots, x_{num\\_patch}]\n",
    "$$\n",
    "\n",
    "* Transformer åœ¨å¤šå±‚ Attention åï¼Œä¼šæ›´æ–°æ‰€æœ‰å‘é‡ï¼Œå…¶ä¸­ `x_cls` èšåˆäº†æ•´ä¸ªå›¾åƒçš„ä¿¡æ¯ã€‚\n",
    "* æœ€ååˆ†ç±»æ—¶ï¼Œåªå– `x_cls` çš„è¾“å‡ºé€å…¥åˆ†ç±»å¤´ (MLP Head)ã€‚"
   ],
   "id": "5777d08998f3b1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,dim,num_heads=8,qkv_bais=False,qk_scale=None,atte_drop_ration=0,proj_drop_ration=0):\n",
    "        # dim è¾“å…¥çš„tokenç»´åº¦ï¼Œ768\n",
    "        # num_heads æ³¨æ„åŠ›å¤´æ•°ï¼Œä¸º8\n",
    "        # qkv_scale ç”Ÿæˆqkvçš„æ—¶å€™æ˜¯å¦æ·»åŠ åç½®\n",
    "        # qkv_scale ç”¨äºç¼©æ”¾qkçš„ç³»æ•°ï¼Œå¦‚æœæ˜¯None,åˆ™ä½¿ç”¨1/sqrt(embed_dim_pre_head)\n",
    "        # atte_drop_out æ³¨æ„åŠ›åˆ†æ•°çš„dropoutçš„æ¯”ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "        # proj_drop_ration æœ€ç»ˆæŠ•å½±å±‚çš„dropoutçš„æ¯”ç‡\n",
    "        super().__init__()\n",
    "        self.num_heads=num_heads # æ³¨æ„åŠ›å¤´æ•°\n",
    "        head_dim=dim//num_heads # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´æ—\n",
    "        self.scale=qk_scale or head_dim**-0.5\n",
    "        self.qkv=nn.Linear(dim,dim*3,bias=qkv_bais) # é€šè¿‡å…¨è¿æ¥å±‚ç”Ÿæˆqkv,ä¸ºäº†å¹¶è¡Œè¿ç®—ï¼Œæé«˜è®¡ç®—æ•ˆç‡ï¼Œå‚æ•°æ›´å¥½\n",
    "        self.att_drop=nn.Dropout(p=atte_drop_ration)\n",
    "        self.proj_drop=nn.Dropout(proj_drop_ration)\n",
    "        # å°†æ¯ä¸ªheadå¾—åˆ°çš„è¾“å‡ºè¿›è¡Œconcatæ‹¼æ¥ï¼Œç„¶åé€šè¿‡çº¿æ€§å˜æ¢æ˜ å°„å›åŸæ¥çš„åµŒå…¥dim\n",
    "        self.proj=nn.Linear(dim,dim)\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        B,N,C=x.shape # B: batch_size N: num_patch+1 C:embed_dim\n",
    "        # qkv(B,N,3*C)=>(B,N,3,num_heads,c//self.num_heads)\n",
    "        qkv=self.qkv(x).reshape(B,N,3,self.num_heads,C//self.num_heads).permute(2,0,3,1,4) # æ–¹ä¾¿åšè¿ç®—\n",
    "        q,k,v=qkv[0],qkv[1],qkv[2]\n",
    "        # è®¡ç®—qkçš„ç‚¹å‡»ï¼Œå¹¶è¿›è¡Œç¼©æ”¾ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°\n",
    "        attn=torch.matmul(q,k.transpose(-2,-1))*self.scale\n",
    "        attn=attn.softmax(dim=-1) # å¯¹æ¯è¡Œè¿›è¡Œsoftmax\n",
    "        # æ³¨æ„åŠ›æƒé‡å¯¹vè¿›è¡ŒåŠ æƒæ±‚å’Œ\n",
    "        # att@v: (B,num_heads,N,c//self.num_heads)\n",
    "        # transpose: (B,N,self.num_heads,c//self.num_heads)\n",
    "        # reshape:(B,N,C)å°†æœ€åä¸¤ä¸ªç»´åº¦ä¿¡æ¯æ‹¼æ¥ï¼Œåˆå¹¶å¤šä¸ªå¤´è¾“å‡ºï¼Œå›åˆ°æ€»çš„åµŒå…¥ç»´åº¦\n",
    "        o=torch.matmul(attn,v).transpose(1,2).reshape(B,N,C)\n",
    "        o=self.proj(o)\n",
    "        o=self.proj_drop(x) # é˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "        return o"
   ],
   "id": "625df3265f4b96fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,in_features,hidden_features=None,out_features=None,act_layer=nn.GELU,drop=0):\n",
    "        # in_features è¾“å…¥æ³•ç»´åº¦\n",
    "        # hidden_features éšè—å±‚çš„ç»´åº¦ï¼Œé€šå¸¸ä¸ºin_featuresçš„4å€ï¼Œout_featuresç»´åº¦é€šå¸¸ä¸ib_featuresç›¸ç­‰\n",
    "        super().__init__()\n",
    "        out_features=out_features or in_features\n",
    "        hidden_features= hidden_features or in_features\n",
    "        self.fc1=nn.Linear(in_features,hidden_features)\n",
    "        self.act=act_layer()\n",
    "        self.fc2=nn.Linear(hidden_features,out_features)\n",
    "        self.drop=nn.Dropout(p=drop)\n",
    "    def forward(self,x):\n",
    "        x=self.fcq(x) # ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚\n",
    "        x=self.act(x) # æ¿€æ´»å‡½æ•°\n",
    "        x=self.drop(x) # ä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    "        x=self.fc2(x)\n",
    "        x=self.drop(x)\n",
    "        return x"
   ],
   "id": "b980e9dd47ead57d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,dim,num_heads,mlp_ratio=4,qkv_bias=False):\n",
    "        super().__init__()\n"
   ],
   "id": "ee0642336936b89a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
